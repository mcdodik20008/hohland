## 1. Метод k-ближайших соседей (KNN)

### Область применения
- Классификация и регрессия.
- Рекомендательные системы, распознавание образов, медицина, финансовые риски.

### Описание метода: суть метода, 4 метрики расстояния, выбор числа – k
- Для предсказания класса/значения нового объекта ищет **k ближайших соседей** по обучающей выборке и решает по их "голосам" (класс) или среднему (регрессия).
- Не требует обучения — хранит всю обучающую выборку ("ленивая модель").

#### Метрики расстояния:
1. **Евклидово (L2):**
   $$
   d_2(a, b) = \sqrt{\sum_{i=1}^m (a_i - b_i)^2}
   $$
2. **Манхэттенское (L1):**
   $$
   d_1(a, b) = \sum_{i=1}^m |a_i - b_i|
   $$
3. **Чебышёвское (L∞):**
   $$
   d_\infty(a, b) = \max_i |a_i - b_i|
   $$
4. **Минковского (Lp):**
   $$
   d_p(a, b) = \left( \sum_{i=1}^m |a_i - b_i|^p \right)^{1/p}
   $$

#### Выбор числа k
- k — гиперпараметр, подбирается кросс-валидацией.
- Малое k: чувствителен к шуму, большое k: сглаживает детали, может недообучаться.

### Гиперпараметры
- **k** — количество соседей.
- **Метрика расстояния** (euclidean, manhattan и др.).
- **Вес соседей** (uniform, distance-based).

### Сравните и сопоставьте методы k-ближайших соседей (KNN) и опорных векторов (SVM)
(см. таблицу из билета №3)

---

## 2. Что такое переобучение в машинном обучении? Как его недопустить?

### Переобучение (Overfitting)
- Модель слишком хорошо запоминает обучающую выборку, теряя способность обобщать на новых данных.
- Признак — высокая точность на обучении и низкая на тесте.

#### Как предотвратить:
- Кросс-валидация.
- Регуляризация (L1, L2).
- Уменьшение сложности модели.
- Сбор большего объёма данных.
- Раннее прекращение обучения (early stopping).

---

### Какова цель регуляризации в машинном обучении? Примеры методов

**Цель:**  
- Борьба с переобучением, ограничение сложности модели, увеличение её обобщающей способности.

**Примеры методов:**
- **L1-регуляризация (Lasso):** штраф за сумму абсолютных значений коэффициентов.
- **L2-регуляризация (Ridge):** штраф за сумму квадратов коэффициентов.
- **Dropout** (в нейронных сетях): случайное обнуление нейронов при обучении.
- **Early stopping**: остановка обучения при ухудшении метрик на валидации.
