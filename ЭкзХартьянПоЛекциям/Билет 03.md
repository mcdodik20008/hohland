## 1. Метод опорных векторов (SVM)

### Область применения
- Классификация (бинарная и многоклассовая), регрессия, обнаружение выбросов.
- Применяется в распознавании изображений, текстов, биоинформатике и др., особенно когда данных мало и признаков много.

### Описание метода

#### Определение гиперплоскости и формула
- Идея SVM — найти такую гиперплоскость, которая **максимально разделяет** классы с максимальным зазором (margin).
- Уравнение гиперплоскости:
  $$
  w^T x + b = 0
  $$
  где \( w \) — вектор весов, \( x \) — вектор признаков, \( b \) — смещение.

#### Классификатор максимального зазора
- Выбирается гиперплоскость с максимальным расстоянием до ближайших точек обоих классов (support vectors).

#### Классификатор опорных векторов
- Только опорные векторы влияют на положение разделяющей гиперплоскости.

#### Kernel trick (ядерный трюк)
- SVM может строить **нелинейные границы**, применяя kernel-функции для перехода в более высокое пространство признаков, где классы линейно разделимы.
- Популярные ядра: линейное, RBF (гауссовское), полиномиальное, сигмоидальное.

### Гиперпараметры
- **C** — штраф за ошибку (чем больше C, тем меньше ошибок на обучении, но выше риск переобучения).
- **Kernel** — тип ядра (linear, rbf, poly, sigmoid).
- **Gamma** — радиус влияния для RBF и других нелинейных ядер.
- **Degree** — степень полинома для полиномиального ядра.

### Сравните и сопоставьте методы k-ближайших соседей (KNN) и опорных векторов (SVM)
|                       | **KNN**                                           | **SVM**                                      |
|-----------------------|---------------------------------------------------|----------------------------------------------|
| Обучение              | Быстрое, без построения явной модели              | Медленно, требует оптимизации                |
| Предсказание          | Медленно (ищет соседей для каждого объекта)       | Быстро (подсчёт скалярного произведения)     |
| Зависимость от размера| Плохо масштабируется с ростом данных              | Хорошо работает с большим количеством признаков |
| Границы               | Линейные и нелинейные, задаются соседями          | Линейные/нелинейные, управляются ядрами      |
| Масштабирование       | Крайне важно                                      | Очень важно                                 |
| Интерпретируемость    | Прост в понимании                                 | Сложнее интерпретировать                     |

---

## 2. Каковы основные различия между обучением с учителем и обучением без учителя?

### Обучение с учителем (Supervised Learning)
- Данные размечены (есть входы и известные ответы/метки).
- Цель — научиться предсказывать метку по признакам.
- Примеры: классификация, регрессия.

### Обучение без учителя (Unsupervised Learning)
- Данные **без разметки** (только признаки).
- Цель — выявить структуру в данных (кластеры, связи, аномалии).
- Примеры: кластеризация, понижение размерности, обнаружение аномалий.

---

### Опишите этапы процесса выбора признаков и объясните их значение.

1. **Сбор и предварительная обработка данных**
2. **Генерация новых признаков (feature engineering)**
3. **Оценка значимости признаков** (корреляция, важность по модели, статистические тесты)
4. **Отбор признаков** (фильтрация, wrapper-методы, embedded-методы)
5. **Валидация отбора** (оценка на кросс-валидации, сравнение качества)
6. **Упрощение модели** (оставляем только важные и информативные признаки)

**Значение:**  
- Уменьшение переобучения, повышение обобщающей способности, ускорение работы моделей и повышение интерпретируемости.
